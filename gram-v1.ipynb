{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5160ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchinfo import summary\n",
    "from torchviz import make_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b6506b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x20158f9ba30>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set seed\n",
    "seed = 24\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46ab9550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Statistics of base dataset (dev=False):\n",
      "\t- Dataset: MIMIC3Dataset\n",
      "\t- Number of patients: 49993\n",
      "\t- Number of visits: 52769\n",
      "\t- Number of visits per patient: 1.0555\n",
      "\t- Number of events per visit in DIAGNOSES_ICD: 9.1038\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nStatistics of base dataset (dev=False):\\n\\t- Dataset: MIMIC3Dataset\\n\\t- Number of patients: 49993\\n\\t- Number of visits: 52769\\n\\t- Number of visits per patient: 1.0555\\n\\t- Number of events per visit in DIAGNOSES_ICD: 9.1038\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyhealth.datasets import MIMIC3Dataset\n",
    "\n",
    "mimic3_ds = MIMIC3Dataset(\n",
    "        root=\"https://storage.googleapis.com/pyhealth/Synthetic_MIMIC-III/\",\n",
    "        tables=[\"DIAGNOSES_ICD\"])\n",
    "\n",
    "# we show the statistics below.\n",
    "mimic3_ds.stat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3fbe04a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Statistics for ICD9CM:\n",
      "\t- Number of nodes: 17736\n",
      "\t- Number of edges: 17733\n",
      "\t- Available attributes: ['name']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyhealth.medcode import InnerMap\n",
    "icd9cm = InnerMap.load(\"ICD9CM\")\n",
    "icd9cm.stat()\n",
    "from pyhealth.medcode import CrossMap\n",
    "codemap = CrossMap.load(\"ICD9CM\", \"CCSCM\")\n",
    "# ccscm = InnerMap.load(\"CCSCM\")\n",
    "# ccscm.stat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a141ed67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def proc_code(code):\n",
    "    if code[0] == 'E':\n",
    "        if len(code) > 4 and len(code) < 8:\n",
    "            return (code[:4] + '.' + code[4:])\n",
    "        else:\n",
    "            return code\n",
    "    else:\n",
    "        if len(code) > 3 and len(code) < 7:\n",
    "            return (code[:3] + '.' + code[3:])\n",
    "        else:\n",
    "            return code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6a84de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leaf codes: 2987\n",
      "ancester codes: 1612\n",
      "# of patients: 2339\n",
      "# of codes: 38767\n",
      "code per visit: 7.579081133919844\n",
      "visit per patient 2.1868319794784097\n",
      "max visit per patient: 7\n",
      "max code per visit: 63\n",
      "# of ccs codes 259\n"
     ]
    }
   ],
   "source": [
    "diag_codes = set()\n",
    "pa = pc = 0\n",
    "codes = []\n",
    "visits = 0\n",
    "leaf_codes = set()\n",
    "ancester_codes = set()\n",
    "ccs_codes = set()\n",
    "ccs_codes_count = []\n",
    "ancester = None\n",
    "max_codes_per_visit = 0\n",
    "max_visits = 0\n",
    "for pid in mimic3_ds.patients:\n",
    "    pa +=1\n",
    "    max_visits = max(max_visits, len(mimic3_ds.patients[pid].visits))\n",
    "    if len(mimic3_ds.patients[pid].visits) > 1:\n",
    "        pc += 1\n",
    "#        print(\"patients:\", pid)\n",
    "        for vid in mimic3_ds.patients[pid].visits:\n",
    "            visits += 1\n",
    "#            print(\"visit:\", vid)\n",
    "            max_codes_per_visit = max(max_codes_per_visit, len(mimic3_ds.patients[pid].visits[vid].get_code_list(\"DIAGNOSES_ICD\")))\n",
    "            for code in mimic3_ds.patients[pid].visits[vid].get_code_list(\"DIAGNOSES_ICD\"):\n",
    "                code = proc_code(code)\n",
    "#                print(\"diagnoses:\", code)\n",
    "                if code in icd9cm:\n",
    "                    diag_codes.add(code)\n",
    "                    leaf_codes.add(code)\n",
    "                    ancesters = icd9cm.get_ancestors(code)\n",
    "#                    print(\"ancesters:\", ancesters)\n",
    "                    for acode in (ancesters):\n",
    "                        ancester_codes.add(acode)\n",
    "                    codes.append(code)\n",
    "#                     print(code, codemap.map(code))\n",
    "                    ccs_codes.add(codemap.map(code)[0])\n",
    "                    ccs_codes_count.append(codemap.map(code)[0])\n",
    "print(\"leaf codes:\", len(leaf_codes))\n",
    "print(\"ancester codes:\", len(ancester_codes))\n",
    "print(\"# of patients:\", pc)\n",
    "print(\"# of codes:\", len(codes))\n",
    "print(\"code per visit:\", len(codes)/visits)\n",
    "print(\"visit per patient\", visits/pc)\n",
    "print(\"max visit per patient:\", max_visits)\n",
    "print(\"max code per visit:\", max_codes_per_visit)\n",
    "print(\"# of ccs codes\", len(ccs_codes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13ea28ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "code_freq = dict(Counter(code for code in codes))\n",
    "code_freq = sorted(code_freq.items(), key=lambda kv: kv[1])\n",
    "n_code = len(code_freq)\n",
    "n_code, code_freq\n",
    "code_freq_dict = {}\n",
    "for i in range(len(code_freq)):\n",
    "    code_freq_dict[code_freq[i][0]] = int((i/len(code_freq))*5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56a90e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "ccs_code_freq = dict(Counter(code for code in ccs_codes_count))\n",
    "ccs_code_freq = sorted(ccs_code_freq.items(), key=lambda kv: kv[1])\n",
    "n_ccs_code = len(ccs_code_freq)\n",
    "ccs_code_freq_dict = {}\n",
    "for i in range(len(ccs_code_freq)):\n",
    "    ccs_code_freq_dict[ccs_code_freq[i][0]] = int((i/len(ccs_code_freq))*5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4eabed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "leaf_ancester_codes = leaf_codes & ancester_codes\n",
    "pure_leaf_codes = leaf_codes - leaf_ancester_codes\n",
    "all_codes = leaf_codes | ancester_codes\n",
    "n_leaf_codes = len(leaf_codes)\n",
    "n_all_codes = len(all_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d58c29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_codes_ordered = list(pure_leaf_codes) + list(leaf_ancester_codes) + list(ancester_codes - leaf_ancester_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20c9e22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_code_dict = {all_codes_ordered[i]:i for i in range(len(all_codes_ordered))}\n",
    "diag_code_dict = {list(diag_codes)[i]:i for i in range(len(diag_codes))}\n",
    "n_diag_codes = len(diag_code_dict)\n",
    "ccs_codes_list = list(ccs_codes)\n",
    "ccs_codes_dict = {ccs_codes_list[i]:i for i in range(len(ccs_codes_list))}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6385b0a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average number of label codes: 0.0\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Now build the attention mask: the mask has a value of 1 if there is an ancester relationship, 0 otherwise.\n",
    "#\n",
    "seqs = []\n",
    "labels = []\n",
    "n_ancesters = 6\n",
    "att_mask = torch.zeros((len(leaf_codes), n_ancesters))\n",
    "att_mask_1 = torch.zeros((n_leaf_codes, n_all_codes))\n",
    "n_labels = 0\n",
    "ancesters_dict = {}\n",
    "for pid in mimic3_ds.patients:\n",
    "    seq = []\n",
    "    label = []\n",
    "    if len(mimic3_ds.patients[pid].visits) > 1:\n",
    "        for vid in mimic3_ds.patients[pid].visits:\n",
    "            icd_codes = mimic3_ds.patients[pid].visits[vid].get_code_list(\"DIAGNOSES_ICD\")\n",
    "            processed_icd_codes = []\n",
    "            for code in icd_codes:\n",
    "                code = proc_code(code)\n",
    "                if code in icd9cm:\n",
    "                    ancesters = icd9cm.get_ancestors(code)\n",
    "                    ancesters_dict[code] = [code] + ancesters\n",
    "                    att_mask_1[all_code_dict[code], all_code_dict[code]] = 1\n",
    "                    for idx_ancester in range(len(ancesters)):\n",
    "                        att_mask[all_code_dict[code], idx_ancester] = 1\n",
    "                        att_mask_1[all_code_dict[code], all_code_dict[ancesters[idx_ancester]]] = 1\n",
    "                    processed_icd_codes.append(all_code_dict[code])\n",
    "            if len(processed_icd_codes) > 0:\n",
    "                seq.append(processed_icd_codes)\n",
    "        if len(seq) > 1:\n",
    "            labels.append([[ccs_codes_dict[codemap.map(all_codes_ordered[code])[0]] for code in codes] for codes in seq[1:]])\n",
    "            seqs.append(seq[:-1])\n",
    "\n",
    "print(\"average number of label codes:\", n_labels/len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "561f46aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2085\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class CustomDataset(Dataset):   \n",
    "    def __init__(self, seqs, labels):\n",
    "        self.x = seqs\n",
    "        self.y = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "\n",
    "dataset = CustomDataset(seqs, labels)\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70048419",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    \"\"\"\n",
    "    TODO: Collate the the list of samples into batches. For each patient, you need to pad the diagnosis\n",
    "        sequences to the sample shape (max # visits, max # diagnosis codes). The padding infomation\n",
    "        is stored in `mask`.\n",
    "    \n",
    "    Arguments:\n",
    "        data: a list of samples fetched from `CustomDataset`\n",
    "        \n",
    "    Outputs:\n",
    "        x: a tensor of shape (# patiens, max # visits, max # diagnosis codes) of type torch.long\n",
    "        masks: a tensor of shape (# patiens, max # visits, max # diagnosis codes) of type torch.bool\n",
    "        rev_x: same as x but in reversed time. This will be used in our RNN model for masking \n",
    "        rev_masks: same as mask but in reversed time. This will be used in our RNN model for masking\n",
    "        y: a tensor of shape (# patiens) of type torch.float\n",
    "        \n",
    "    Note that you can obtains the list of diagnosis codes and the list of hf labels\n",
    "        using: `sequences, labels = zip(*data)`\n",
    "    \"\"\"\n",
    "\n",
    "    sequences, labels = zip(*data)\n",
    "\n",
    "\n",
    "    num_patients = len(sequences)\n",
    "    num_visits = [len(patient) for patient in sequences]\n",
    "    num_codes = [len(visit) for patient in sequences for visit in patient]\n",
    "\n",
    "    max_num_visits = max(num_visits)\n",
    "    max_num_codes = max(num_codes)\n",
    "    \n",
    "    x = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.long)\n",
    "    rev_x = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.long)\n",
    "    masks = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.bool)\n",
    "    rev_masks = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.bool)\n",
    "    for i_patient, patient in enumerate(sequences):\n",
    "        for j_visit, visit in enumerate(patient):\n",
    "            x[i_patient][j_visit][range(len(sequences[i_patient][j_visit]))] = torch.LongTensor(sequences[i_patient][j_visit])\n",
    "            masks[i_patient][j_visit][range(len(sequences[i_patient][j_visit]))] = True\n",
    "            rev_x[i_patient][num_visits[i_patient] - 1 - j_visit][range(len(sequences[i_patient][j_visit]))] = torch.LongTensor(sequences[i_patient][j_visit])\n",
    "            rev_masks[i_patient][num_visits[i_patient] - 1 - j_visit][range(len(sequences[i_patient][j_visit]))] = True\n",
    "\n",
    "                        \n",
    "    y = torch.zeros((num_patients, max_num_visits, n_ccs_code), dtype = torch.float) # n_leaf_codes), dtype=torch.float)\n",
    "    for i_patient, visits in enumerate(labels):\n",
    "        for i_visit, codes in enumerate(visits):\n",
    "            for code in codes:\n",
    "                y[i_patient][i_visit][code] = 1\n",
    "            \n",
    "    return x, masks, rev_x, rev_masks, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4c488a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=10, collate_fn=collate_fn)\n",
    "loader_iter = iter(loader)\n",
    "x, masks, rev_x, rev_masks, y = next(loader_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8a23e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train dataset: 1668\n",
      "Length of val dataset: 417\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "split = int(len(dataset)*0.8)\n",
    "lengths = [split, len(dataset) - split]\n",
    "train_dataset, val_dataset = random_split(dataset, lengths)\n",
    "\n",
    "print(\"Length of train dataset:\", len(train_dataset))\n",
    "print(\"Length of val dataset:\", len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c11167e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def load_data(train_dataset, val_dataset, collate_fn):\n",
    "    \n",
    "    '''\n",
    "    TODO: Implement this function to return the data loader for  train and validation dataset. \n",
    "    Set batchsize to 32. Set `shuffle=True` only for train dataloader.\n",
    "    \n",
    "    Arguments:\n",
    "        train dataset: train dataset of type `CustomDataset`\n",
    "        val dataset: validation dataset of type `CustomDataset`\n",
    "        collate_fn: collate function\n",
    "        \n",
    "    Outputs:\n",
    "        train_loader, val_loader: train and validation dataloaders\n",
    "    \n",
    "    Note that you need to pass the collate function to the data loader `collate_fn()`.\n",
    "    '''\n",
    "    \n",
    "    batch_size = 2000\n",
    "    train_loader = DataLoader(train_dataset, batch_size = batch_size, collate_fn = collate_fn, shuffle = True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size = batch_size, collate_fn = collate_fn, shuffle = False)\n",
    "        \n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "train_loader, val_loader = load_data(train_dataset, val_dataset, collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ce3fc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_embeddings_with_mask(x, masks):\n",
    "    \"\"\"\n",
    "    TODO: mask select the embeddings for true visits (not padding visits) and then\n",
    "        sum the embeddings for each visit up.\n",
    "\n",
    "    Arguments:\n",
    "        x: the embeddings of diagnosis sequence of shape (batch_size, # visits, # diagnosis codes, embedding_dim)\n",
    "        masks: the padding masks of shape (batch_size, # visits, # diagnosis codes)\n",
    "\n",
    "    Outputs:\n",
    "        sum_embeddings: the sum of embeddings of shape (batch_size, # visits, embedding_dim)\n",
    "        \n",
    "    NOTE: Do NOT use for loop.\n",
    "\n",
    "    \"\"\"\n",
    "    masked_x = torch.mul(x, masks[:,:,:,None])\n",
    "    return torch.sum(masked_x, 2, keepdim=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a649477b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_visit(hidden_states, masks):\n",
    "    \"\"\"\n",
    "    TODO: obtain the hidden state for the last true visit (not padding visits)\n",
    "\n",
    "    Arguments:\n",
    "        hidden_states: the hidden states of each visit of shape (batch_size, # visits, embedding_dim)\n",
    "        masks: the padding masks of shape (batch_size, # visits, # diagnosis codes)\n",
    "\n",
    "    Outputs:\n",
    "        last_hidden_state: the hidden state for the last true visit of shape (batch_size, embedding_dim)\n",
    "        \n",
    "    NOTE: DO NOT use for loop.\n",
    "    \n",
    "    HINT: First convert the mask to a vector of shape (batch_size,) containing the true visit length; \n",
    "          and then use this length vector as index to select the last visit.\n",
    "    \"\"\"\n",
    "    \n",
    "    # your code here\n",
    "    last_visit = torch.sum((torch.sum(masks, 2, keepdim = False) > 0).int(), 1)\n",
    "    return hidden_states[range(hidden_states.shape[0]),last_visit-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b3b819b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score, top_k_accuracy_score\n",
    "\n",
    "def eval_model(model, val_loader):\n",
    "    \n",
    "    \"\"\"\n",
    "    TODO: evaluate the model.\n",
    "    \n",
    "    Arguments:\n",
    "        model: the RNN model\n",
    "        val_loader: validation dataloader\n",
    "        \n",
    "    Outputs:\n",
    "        precision: overall precision score\n",
    "        recall: overall recall score\n",
    "        f1: overall f1 score\n",
    "        roc_auc: overall roc_auc score\n",
    "        \n",
    "    Note that please pass all four arguments to the model so that we can use this function for both \n",
    "    models. (Use `model(x, masks, rev_x, rev_masks)`.)\n",
    "        \n",
    "    HINT: checkout https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics\n",
    "    \"\"\"\n",
    "    k = 20\n",
    "    model.eval()\n",
    "    y_pred = torch.LongTensor()\n",
    "    y_score = torch.Tensor()\n",
    "    y_true = torch.LongTensor()\n",
    "    model.eval()\n",
    "    found_all = 0\n",
    "    actual_all = 0 \n",
    "    samples = 0\n",
    "    n_freq_band = 5\n",
    "    top_k_tp = torch.zeros(n_freq_band, dtype=torch.float)\n",
    "    top_k_fn = torch.zeros(n_freq_band, dtype=torch.float)\n",
    "    top_k_fp = torch.zeros(n_freq_band, dtype=torch.float)\n",
    "    top_k_tn = torch.zeros(n_freq_band, dtype=torch.float)\n",
    "    for x, masks, rev_x, rev_masks, y in val_loader:\n",
    "        y_hat = model(x, masks, rev_x, rev_masks).detach()\n",
    "        for i in range(len(y_hat)):\n",
    "            for m in range(len(y_hat[0])):\n",
    "                top_k = np.argpartition(y_hat[i][m].detach(), -k)[-k:]\n",
    "                for j in range(n_ccs_code): # n_leaf_codes):\n",
    "                    if(y[i][m][j] > 0.5):\n",
    "                        # code = all_codes_ordered[j]\n",
    "                        # print(\"actual code\", code)\n",
    "                        if j in top_k:\n",
    "                            # print(code, \"in top \", code_freq_dict[code], \"percentile\")\n",
    "                            # top_k_tp[code_freq_dict[code]] += 1\n",
    "                            top_k_tp[ccs_code_freq_dict[ccs_codes_list[j]]] += 1\n",
    "                        else:\n",
    "                            # top_k_fn[code_freq_dict[code]] += 1\n",
    "                            top_k_fn[ccs_code_freq_dict[ccs_codes_list[j]]] += 1\n",
    "    print(top_k_tp, top_k_fn)\n",
    "    return (top_k_tp/(top_k_tp+top_k_fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fc9bc875",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train(model, train_loader, val_loader, n_epochs):\n",
    "    \"\"\"\n",
    "    TODO: train the model.\n",
    "    \n",
    "    Arguments:\n",
    "        model: the RNN model\n",
    "        train_loader: training dataloder\n",
    "        val_loader: validation dataloader\n",
    "        n_epochs: total number of epochs\n",
    "        \n",
    "    You need to call `eval_model()` at the end of each training epoch to see how well the model performs \n",
    "    on validation data.\n",
    "        \n",
    "    Note that please pass all four arguments to the model so that we can use this function for both \n",
    "    models. (Use `model(x, masks, rev_x, rev_masks)`.)\n",
    "    \"\"\"\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        start_time = time.time()\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for x, masks, rev_x, rev_masks, y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            y_hat = model(x, masks, rev_x, rev_masks)\n",
    "#             print(\"y_hat size\", y_hat.size())\n",
    "#             print(\"y size\", y.size())\n",
    "            loss = criterion(y_hat, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        print('Epoch: {} \\t Training Loss: {:.6f}'.format(epoch+1, train_loss))\n",
    "       # make_dot(y_hat, params=dict(model.named_parameters()))\n",
    "        end_time = time.time()\n",
    "        print(\"time in this epoch\", end_time-start_time)\n",
    "        top_k_acc = eval_model(model, val_loader)\n",
    "        print(\"top k acc \", top_k_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693ea3a0",
   "metadata": {},
   "source": [
    "# RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8abc4ac4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NaiveRNN(\n",
       "  (embedding): Embedding(2987, 128)\n",
       "  (rnn): GRU(128, 128, batch_first=True)\n",
       "  (rev_rnn): GRU(128, 128, batch_first=True)\n",
       "  (fc): Linear(in_features=256, out_features=259, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NaiveRNN(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    TODO: implement the naive RNN model above.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_codes):\n",
    "        super().__init__()\n",
    "        embDimSize = 128\n",
    "        hidden_size = 128\n",
    "        self.embedding = nn.Embedding(num_codes, embDimSize)\n",
    "        self.rnn = nn.GRU(input_size = embDimSize, hidden_size = hidden_size, batch_first = True)\n",
    "        self.rev_rnn = nn.GRU(input_size = embDimSize, hidden_size = hidden_size, batch_first = True)\n",
    "        self.fc = nn.Linear(hidden_size * 2, n_ccs_code) # n_diag_codes)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    \n",
    "    def forward(self, x, masks, rev_x, rev_masks):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: the diagnosis sequence of shape (batch_size, # visits, # diagnosis codes)\n",
    "            masks: the padding masks of shape (batch_size, # visits, # diagnosis codes)\n",
    "\n",
    "        Outputs:\n",
    "            probs: probabilities of shape (batch_size, # diagnosis codes)\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        x = self.embedding(x)\n",
    "        x = sum_embeddings_with_mask(x, masks)\n",
    "        output, _ = self.rnn(x)\n",
    "        true_h_n = output # get_last_visit(output, masks)\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO:\n",
    "            5. Do the step 1-4 again for the reverse order (rev_x), and concatenate the hidden\n",
    "               states for both directions;\n",
    "        \"\"\"\n",
    "    \n",
    "        rev_x = self.embedding(rev_x)\n",
    "        # 5b. Sum the embeddings for each diagnosis code up for a visit of a patient.\n",
    "        rev_x = sum_embeddings_with_mask(rev_x, rev_masks)\n",
    "        # 5c. Pass the embegginds through the RNN layer;\n",
    "        rev_output, _ = self.rev_rnn(rev_x)\n",
    "        # 5d. Obtain the hidden state at the last visit.\n",
    "        true_h_n_rev = rev_output # get_last_visit(rev_output, rev_masks)        \n",
    "        # 6. Pass the hidden state through the linear and activation layers.\n",
    "        logits = self.fc(torch.cat([true_h_n, true_h_n_rev], 2))\n",
    "        probs = self.sigmoid(logits)\n",
    "        return probs\n",
    "    \n",
    "\n",
    "# load the model here\n",
    "naive_rnn = NaiveRNN(num_codes = n_leaf_codes)\n",
    "naive_rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0ded9605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "NaiveRNN                                 --\n",
       "├─Embedding: 1-1                         382,336\n",
       "├─GRU: 1-2                               99,072\n",
       "├─GRU: 1-3                               99,072\n",
       "├─Linear: 1-4                            66,563\n",
       "├─Sigmoid: 1-5                           --\n",
       "=================================================================\n",
       "Total params: 647,043\n",
       "Trainable params: 647,043\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(naive_rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bae01673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \t Training Loss: 0.034411\n",
      "time in this epoch 1.9839236736297607\n",
      "tensor([0.0000e+00, 0.0000e+00, 1.0000e+00, 1.6000e+01, 1.1030e+03]) tensor([  17.,   67.,  183.,  452., 1115.])\n",
      "top k acc  tensor([0.0000, 0.0000, 0.0054, 0.0342, 0.4973])\n",
      "Epoch: 2 \t Training Loss: 0.028249\n",
      "time in this epoch 1.8102672100067139\n",
      "tensor([   0.,    0.,    0.,   13., 1142.]) tensor([  17.,   67.,  184.,  455., 1076.])\n",
      "top k acc  tensor([0.0000, 0.0000, 0.0000, 0.0278, 0.5149])\n",
      "Epoch: 3 \t Training Loss: 0.024460\n",
      "time in this epoch 1.9080932140350342\n",
      "tensor([   0.,    0.,    0.,    5., 1169.]) tensor([  17.,   67.,  184.,  463., 1049.])\n",
      "top k acc  tensor([0.0000, 0.0000, 0.0000, 0.0107, 0.5271])\n",
      "Epoch: 4 \t Training Loss: 0.023011\n",
      "time in this epoch 1.8099751472473145\n",
      "tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 1.2620e+03]) tensor([ 17.,  67., 184., 467., 956.])\n",
      "top k acc  tensor([0.0000, 0.0000, 0.0000, 0.0021, 0.5690])\n",
      "Epoch: 5 \t Training Loss: 0.023128\n",
      "time in this epoch 1.9748694896697998\n",
      "tensor([   0.,    0.,    0.,    0., 1285.]) tensor([ 17.,  67., 184., 468., 933.])\n",
      "top k acc  tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.5794])\n",
      "Epoch: 6 \t Training Loss: 0.022850\n",
      "time in this epoch 1.7639715671539307\n",
      "tensor([   0.,    0.,    0.,    0., 1300.]) tensor([ 17.,  67., 184., 468., 918.])\n",
      "top k acc  tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.5861])\n",
      "Epoch: 7 \t Training Loss: 0.021869\n",
      "time in this epoch 1.9025249481201172\n",
      "tensor([   0.,    0.,    0.,    0., 1298.]) tensor([ 17.,  67., 184., 468., 920.])\n",
      "top k acc  tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.5852])\n",
      "Epoch: 8 \t Training Loss: 0.021011\n",
      "time in this epoch 1.9097909927368164\n",
      "tensor([   0.,    0.,    0.,    0., 1298.]) tensor([ 17.,  67., 184., 468., 920.])\n",
      "top k acc  tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.5852])\n",
      "Epoch: 9 \t Training Loss: 0.020599\n",
      "time in this epoch 1.7730188369750977\n",
      "tensor([   0.,    0.,    0.,    0., 1303.]) tensor([ 17.,  67., 184., 468., 915.])\n",
      "top k acc  tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.5875])\n",
      "Epoch: 10 \t Training Loss: 0.020477\n",
      "time in this epoch 1.8532164096832275\n",
      "tensor([   0.,    0.,    0.,    0., 1296.]) tensor([ 17.,  67., 184., 468., 922.])\n",
      "top k acc  tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.5843])\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(naive_rnn.parameters(), lr = 0.01)\n",
    "# number of epochs to train the model\n",
    "n_epochs = 10\n",
    "train(naive_rnn, train_loader, val_loader, n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f08dcfe",
   "metadata": {},
   "source": [
    "# GRAM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "34a0a222",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRAM(torch.nn.Module):\n",
    "    def __init__(self, n_leaf_codes, n_all_codes, n_diag_codes, n_emb=128, l=64):\n",
    "        super(GRAM, self).__init__()\n",
    "        self.n_emb = n_emb # size of code embedding, m in the paper\n",
    "        self.l = l # size of hidden layer\n",
    "        self.n_ancesters = 6\n",
    "        self.n_hidden = 128\n",
    "        self.n_leaf_codes = n_leaf_codes\n",
    "        self.n_all_codes = n_all_codes\n",
    "        #\n",
    "        # initialzie embedding matrix to random.  Future work: use Glove to train embedding.\n",
    "        #\n",
    "        self.E = nn.Parameter(torch.rand(self.n_all_codes, self.n_emb))\n",
    "\n",
    "        embDimSize = self.n_emb\n",
    "        hidden_size = self.n_hidden\n",
    "       \n",
    "        self.fc1 = nn.Linear(2*self.n_emb, self.l)  # Wa, b_a\n",
    "        self.fc2 = nn.Linear(self.l, 1, bias = False)   # ua\n",
    "        \n",
    "        self.rnn = nn.GRU(input_size = embDimSize, hidden_size = hidden_size, batch_first = True)\n",
    "        self.rev_rnn = nn.GRU(input_size = embDimSize, hidden_size = hidden_size, batch_first = True)\n",
    "        self.fc = nn.Linear(hidden_size * 2, n_ccs_code) # n_diag_codes)  # W, b\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def update_G(self):\n",
    "        a1 = torch.zeros((self.n_leaf_codes, self.n_all_codes))\n",
    "        for child in range(self.n_leaf_codes):\n",
    "            if child % 400 == 0:\n",
    "                print(\"processing leaf node\", child)\n",
    "            for parent in range(self.n_all_codes):\n",
    "                if att_mask_1[child][parent]:\n",
    "                    eij = torch.cat([self.E[child], self.E[parent]])\n",
    "                    a1[child][parent] = self.fc2(torch.tanh(self.fc1(eij)))        \n",
    "        a2 = torch.mul(nn.Softmax(dim=1)(a1), att_mask_1)\n",
    "        a = (a2/torch.sum(a2, dim=1, keepdim=True))\n",
    "        G = torch.matmul(a, self.E)  # G shape: n_leaf_node x n_emb\n",
    "        return G\n",
    "    \n",
    "    def embedding(self, x, G):\n",
    "        v = torch.tanh(torch.matmul(x.float(), G))\n",
    "        return v\n",
    "                                                         \n",
    "    def forward(self, x, masks, rev_x, rev_masks):\n",
    "        batch_size = x.shape[0]\n",
    "        G = self.update_G()\n",
    "        \n",
    "        x = nn.functional.one_hot(x, num_classes = n_diag_codes).sum(dim=2)\n",
    "        x_emb = self.embedding(x, G)\n",
    "        output, _ = self.rnn(x_emb)\n",
    "        true_h_n = output # get_last_visit(output, masks)\n",
    "        \n",
    "        rev_x = nn.functional.one_hot(rev_x, num_classes = n_diag_codes).sum(dim=2)\n",
    "        rev_x_emb = self.embedding(rev_x, G)\n",
    "        rev_output, _ = self.rev_rnn(rev_x_emb)\n",
    "        true_h_n_rev = rev_output # get_last_visit(rev_output, rev_masks)\n",
    "        \n",
    "        logits = self.fc(torch.cat([true_h_n, true_h_n_rev], 2))\n",
    "        probs = self.sigmoid(logits)\n",
    "        return probs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cf853398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2987 4543 2987\n",
      "GRAM(\n",
      "  (fc1): Linear(in_features=256, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=1, bias=False)\n",
      "  (rnn): GRU(128, 128, batch_first=True)\n",
      "  (rev_rnn): GRU(128, 128, batch_first=True)\n",
      "  (fc): Linear(in_features=256, out_features=259, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "GRAM                                     581,504\n",
       "├─Linear: 1-1                            16,448\n",
       "├─Linear: 1-2                            64\n",
       "├─GRU: 1-3                               99,072\n",
       "├─GRU: 1-4                               99,072\n",
       "├─Linear: 1-5                            66,563\n",
       "├─Sigmoid: 1-6                           --\n",
       "=================================================================\n",
       "Total params: 862,723\n",
       "Trainable params: 862,723\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(n_leaf_codes, n_all_codes, n_diag_codes)\n",
    "gram = GRAM(n_leaf_codes, n_all_codes, n_diag_codes)\n",
    "print(gram)\n",
    "summary(gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1bcff1c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing leaf node 0\n",
      "processing leaf node 400\n",
      "processing leaf node 800\n",
      "processing leaf node 1200\n",
      "processing leaf node 1600\n",
      "processing leaf node 2000\n",
      "processing leaf node 2400\n",
      "processing leaf node 2800\n",
      "Epoch: 1 \t Training Loss: 0.694918\n",
      "time in this epoch 554.800961971283\n",
      "processing leaf node 0\n",
      "processing leaf node 400\n",
      "processing leaf node 800\n",
      "processing leaf node 1200\n",
      "processing leaf node 1600\n",
      "processing leaf node 2000\n",
      "processing leaf node 2400\n",
      "processing leaf node 2800\n",
      "tensor([  1.,   7.,   3.,  62., 271.]) tensor([  16.,   60.,  181.,  406., 1947.])\n",
      "top k acc  tensor([0.0588, 0.1045, 0.0163, 0.1325, 0.1222])\n",
      "processing leaf node 0\n",
      "processing leaf node 400\n",
      "processing leaf node 800\n",
      "processing leaf node 1200\n",
      "processing leaf node 1600\n",
      "processing leaf node 2000\n",
      "processing leaf node 2400\n",
      "processing leaf node 2800\n",
      "Epoch: 2 \t Training Loss: 0.366427\n",
      "time in this epoch 574.2729349136353\n",
      "processing leaf node 0\n",
      "processing leaf node 400\n",
      "processing leaf node 800\n",
      "processing leaf node 1200\n",
      "processing leaf node 1600\n",
      "processing leaf node 2000\n",
      "processing leaf node 2400\n",
      "processing leaf node 2800\n",
      "tensor([  1.,   5.,   3.,  59., 293.]) tensor([  16.,   62.,  181.,  409., 1925.])\n",
      "top k acc  tensor([0.0588, 0.0746, 0.0163, 0.1261, 0.1321])\n",
      "processing leaf node 0\n",
      "processing leaf node 400\n",
      "processing leaf node 800\n",
      "processing leaf node 1200\n",
      "processing leaf node 1600\n",
      "processing leaf node 2000\n",
      "processing leaf node 2400\n",
      "processing leaf node 2800\n",
      "Epoch: 3 \t Training Loss: 0.100902\n",
      "time in this epoch 539.604113817215\n",
      "processing leaf node 0\n",
      "processing leaf node 400\n",
      "processing leaf node 800\n",
      "processing leaf node 1200\n",
      "processing leaf node 1600\n",
      "processing leaf node 2000\n",
      "processing leaf node 2400\n",
      "processing leaf node 2800\n",
      "tensor([  1.,   5.,   8.,  60., 293.]) tensor([  16.,   62.,  176.,  408., 1925.])\n",
      "top k acc  tensor([0.0588, 0.0746, 0.0435, 0.1282, 0.1321])\n",
      "processing leaf node 0\n",
      "processing leaf node 400\n",
      "processing leaf node 800\n",
      "processing leaf node 1200\n",
      "processing leaf node 1600\n",
      "processing leaf node 2000\n",
      "processing leaf node 2400\n",
      "processing leaf node 2800\n",
      "Epoch: 4 \t Training Loss: 0.046346\n",
      "time in this epoch 538.9279632568359\n",
      "processing leaf node 0\n",
      "processing leaf node 400\n",
      "processing leaf node 800\n",
      "processing leaf node 1200\n",
      "processing leaf node 1600\n",
      "processing leaf node 2000\n",
      "processing leaf node 2400\n",
      "processing leaf node 2800\n",
      "tensor([  1.,   5.,   3.,  60., 332.]) tensor([  16.,   62.,  181.,  408., 1886.])\n",
      "top k acc  tensor([0.0588, 0.0746, 0.0163, 0.1282, 0.1497])\n",
      "processing leaf node 0\n",
      "processing leaf node 400\n",
      "processing leaf node 800\n",
      "processing leaf node 1200\n",
      "processing leaf node 1600\n",
      "processing leaf node 2000\n",
      "processing leaf node 2400\n",
      "processing leaf node 2800\n",
      "Epoch: 5 \t Training Loss: 0.033846\n",
      "time in this epoch 586.4638438224792\n",
      "processing leaf node 0\n",
      "processing leaf node 400\n",
      "processing leaf node 800\n",
      "processing leaf node 1200\n",
      "processing leaf node 1600\n",
      "processing leaf node 2000\n",
      "processing leaf node 2400\n",
      "processing leaf node 2800\n",
      "tensor([  1.,   5.,   3.,  46., 550.]) tensor([  16.,   62.,  181.,  422., 1668.])\n",
      "top k acc  tensor([0.0588, 0.0746, 0.0163, 0.0983, 0.2480])\n",
      "processing leaf node 0\n",
      "processing leaf node 400\n",
      "processing leaf node 800\n",
      "processing leaf node 1200\n",
      "processing leaf node 1600\n",
      "processing leaf node 2000\n",
      "processing leaf node 2400\n",
      "processing leaf node 2800\n",
      "Epoch: 6 \t Training Loss: 0.031459\n",
      "time in this epoch 596.015059709549\n",
      "processing leaf node 0\n",
      "processing leaf node 400\n",
      "processing leaf node 800\n",
      "processing leaf node 1200\n",
      "processing leaf node 1600\n",
      "processing leaf node 2000\n",
      "processing leaf node 2400\n",
      "processing leaf node 2800\n",
      "tensor([  1.,   5.,   3.,  37., 665.]) tensor([  16.,   62.,  181.,  431., 1553.])\n",
      "top k acc  tensor([0.0588, 0.0746, 0.0163, 0.0791, 0.2998])\n",
      "processing leaf node 0\n",
      "processing leaf node 400\n",
      "processing leaf node 800\n",
      "processing leaf node 1200\n",
      "processing leaf node 1600\n",
      "processing leaf node 2000\n",
      "processing leaf node 2400\n",
      "processing leaf node 2800\n",
      "Epoch: 7 \t Training Loss: 0.031923\n",
      "time in this epoch 591.2192125320435\n",
      "processing leaf node 0\n",
      "processing leaf node 400\n",
      "processing leaf node 800\n",
      "processing leaf node 1200\n",
      "processing leaf node 1600\n",
      "processing leaf node 2000\n",
      "processing leaf node 2400\n",
      "processing leaf node 2800\n",
      "tensor([  0.,   5.,   3.,  19., 878.]) tensor([  17.,   62.,  181.,  449., 1340.])\n",
      "top k acc  tensor([0.0000, 0.0746, 0.0163, 0.0406, 0.3959])\n",
      "processing leaf node 0\n",
      "processing leaf node 400\n",
      "processing leaf node 800\n",
      "processing leaf node 1200\n",
      "processing leaf node 1600\n",
      "processing leaf node 2000\n",
      "processing leaf node 2400\n",
      "processing leaf node 2800\n",
      "Epoch: 8 \t Training Loss: 0.033134\n",
      "time in this epoch 569.2500319480896\n",
      "processing leaf node 0\n",
      "processing leaf node 400\n",
      "processing leaf node 800\n",
      "processing leaf node 1200\n",
      "processing leaf node 1600\n",
      "processing leaf node 2000\n",
      "processing leaf node 2400\n",
      "processing leaf node 2800\n",
      "tensor([   0.,    2.,    0.,   15., 1137.]) tensor([  17.,   65.,  184.,  453., 1081.])\n",
      "top k acc  tensor([0.0000, 0.0299, 0.0000, 0.0321, 0.5126])\n",
      "processing leaf node 0\n",
      "processing leaf node 400\n",
      "processing leaf node 800\n",
      "processing leaf node 1200\n",
      "processing leaf node 1600\n",
      "processing leaf node 2000\n",
      "processing leaf node 2400\n",
      "processing leaf node 2800\n",
      "Epoch: 9 \t Training Loss: 0.034412\n",
      "time in this epoch 587.6370947360992\n",
      "processing leaf node 0\n",
      "processing leaf node 400\n",
      "processing leaf node 800\n",
      "processing leaf node 1200\n",
      "processing leaf node 1600\n",
      "processing leaf node 2000\n",
      "processing leaf node 2400\n",
      "processing leaf node 2800\n",
      "tensor([0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 1.2500e+03]) tensor([ 17.,  66., 184., 468., 968.])\n",
      "top k acc  tensor([0.0000, 0.0149, 0.0000, 0.0000, 0.5636])\n",
      "processing leaf node 0\n",
      "processing leaf node 400\n",
      "processing leaf node 800\n",
      "processing leaf node 1200\n",
      "processing leaf node 1600\n",
      "processing leaf node 2000\n",
      "processing leaf node 2400\n",
      "processing leaf node 2800\n",
      "Epoch: 10 \t Training Loss: 0.035530\n",
      "time in this epoch 571.766872882843\n",
      "processing leaf node 0\n",
      "processing leaf node 400\n",
      "processing leaf node 800\n",
      "processing leaf node 1200\n",
      "processing leaf node 1600\n",
      "processing leaf node 2000\n",
      "processing leaf node 2400\n",
      "processing leaf node 2800\n",
      "tensor([   0.,    0.,    0.,    0., 1301.]) tensor([ 17.,  67., 184., 468., 917.])\n",
      "top k acc  tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.5866])\n"
     ]
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(False)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(gram.parameters(), lr = 0.01)\n",
    "n_epochs = 10\n",
    "train(gram, train_loader, val_loader, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323a463f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gram.E[0,]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d1791a",
   "metadata": {},
   "source": [
    "# Ablation Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27eca12",
   "metadata": {},
   "outputs": [],
   "source": [
    "pure_ancester_codes = list(all_codes - leaf_ancester_codes)\n",
    "n_pure_ancester_code = len(pure_ancester_codes)\n",
    "                           \n",
    "def random_code():\n",
    "    indexes = random.sample(range(n_leaf_codes, n_all_codes), n_ancesters-1)\n",
    "    return indexes\n",
    "\n",
    "att_mask_1 = torch.zeros((n_leaf_codes, n_all_codes))\n",
    "\n",
    "for child in range(n_leaf_codes):\n",
    "    ancesters = random_code()\n",
    "    att_mask_1[child, child] = 1\n",
    "    for ancester in ancesters:\n",
    "        att_mask_1[child,ancester] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523a7e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "att_mask_1[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151a73ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "gram_ablation = GRAM(n_leaf_codes, n_all_codes, n_diag_codes)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(gram_ablation.parameters(), lr = 0.01)\n",
    "n_epochs = 10\n",
    "train(gram_ablation, train_loader, val_loader, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f777f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gram_ablation.E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16481aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "gram.E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955d4f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "g"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
